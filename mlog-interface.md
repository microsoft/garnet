This PR implements sharded-log support for Garnet replication.
This is a draft PR to keep track of the major TODOs still needed.

TODO:
- [X] Ensure that monotonicity of timestamps is fullfilled across failovers
<ul><ul>
What happens in the event the timestamp generated by a replica that became the new primary was behind the primary generated by the old primary?
</ul></ul>
- [X] Ensure replica replay task caches objects accessed during replay (i.e. tsavoriteLog instance)
<ul><ul>
A dedicated session is responsible for processing CLUSTER APPENDLOG commands at the replica.
For each session, we need to cache the log instance associated with the sublogIdx that is provided by the APPENDLOG command.
</ul></ul>

- [X] Build mechanism to coordinate dispose and reset of sublog replica replay task
<ul><ul>
Every sublog replay task will have to be able to signal and cancel other tasks from its group in the event of replication failure.
This requires a mechanism to inform other sublog replay tasks and ensure that they are shutdown or reset before making any new attempts to re-establish the replication stream 
</ul></ul>

- [X] Ensure that the timestampTracker that is used for the read protocol is reset appropriately when a node change its state.
<ul><ul>
This happens in the event a replica starts replicating a new primary
</ul></ul>

- [X] Ensure timestamp tracker recovers correctly alongside the sequence number generator
ReplayProgressTracker
Tracks information about the sequence numbers of the keys replayed from the AOF.
This information is used by RespsServerSessions to perform consistent read.
The consistent read is executed in two phases:
1. Ensure prefix consistency across virtual sublogs 
    To enforce this, we need to compare the sequence number (timestamp) of key being read to the maximum session sequence number (mssn)
    as determined by previous reads.
    The ReplayProgressTracker maintains a replayInfoArray which keeps tracks the sequence numbers for replayed keys for all virtual replay tasks.
    The replayInfoArray maintains a fixed number of slots which are used to track the sequence number of a bag of keys
    Each replay task is responsible for updating the sequence number of a key after replaying the corresponding operation.
    Replay task number is equal or greater to the physical sublog number.
    Hence, updates to replayInfoArray are indexed using (sublogIdx, replayTaskIdx, keyIdx)
    
    The current key sequence number (cksn) is determined as the 

by comparing the sequence number (timestamp) of the key being read
2. Update 

<ul><ul></ul></ul> 
- [X] Validate diskless replication tests and add new tests for replay coordination.
<ul><ul></ul></ul> 

- [X] Implement read protocol
<ul><ul>
The protocol needs to ensure prefix consistency for every session running on the replica.
This is achieved by waiting until the sequence number of the key being read is greater or equal to the maximum session sequence number.
However, this establishes only a lower bound for the actual sequence number. If an update occurs in between the moment the lower bound was established and before the actual read of the data, then the actual sequence number has been underestimated which violates the read protocol by advertising a less restrictive maximum sequence number for the session.
Hence, we update the maximum session sequence number after the actual read to ensure prefix consistency
This is enforced for the following operations

<ul><ul>
- [X] Basic context operations
</ul></ul> 
<ul><ul>
- [X] Scan operations
</ul></ul> 
<ul><ul>
- [X] Transaction operations
</ul></ul> 

The read protocol is implemented using the following

1. RespServerSession.ConsistentReadKeyPrepareCallback: Method to check key sequence number against the maximum session sequence number and wait for the former to catch up.
2. RespServerSession.ConsistentReadKeyPrepareCallback: Method to update maximum session sequence number with the key sequence number.
3. ConsistentReadContext: An extension to the BasicContext that overloads the Read methods, calling ConsistentReadKeyPrepareCallback before read and ConsistentReadKeyUpdateCallback after read
4. ConsistentReadDatabaseSession: A GarnetDatabaseSession instance initialized with the ConsistenReadContext instance

The main idea is to maintain two database sessions per RespServerSession instances. Primary nodes server read/writes on the default database session.
Replicas will use the ConsistentReadDatabaseSession to server consistent reads when sharded-log based AOF is enabled in cluster modes.
We require two distinct database sessions to allow any given node to switch back and forth between being a primary and a replica.
The primaries avoid evaluation of the consistent read protocol completely through this approach.

NOTES: 
1. The database session switch happens when the node transitions from primary to replica and vice versa.
This will happen at CLUSTER REPLICATE, REPLICAOF or CLUSTER FAILOVER

2. If the read goes pending the update happens after the complete pending completes.
This should guarantee prefix consistency even in the scatter-gather case.
This realies on the assumption that the read returns the unaltered disk value from the immutable region,
otherwise an RMW from the replay thread could interject an newer value which is not consistent according to the protocol.
Good thing is that this is what currently happens on the complete pending callback (Need to actually verify).

</ul></ul> 

- [X] Re-design tail witness to avoid using enqueue.
- [ ] Implement logical subtask replay (LSR)
<ul><ul>
Garnet replication can be configured to utilize multiple sublogs at the primary and many virtual replay tasks at the replica.
At minimum one can deploy Garnet with single log and single virtual replay task.
This case will be similar to what we have today and which does not need any additional information in the AofHeader.
For AofSublogCount > 1 and/or AofReplaySubtaskCount > 1, we need to make use AofExtendedHeader to store the following additional information

1. Timestamp/Sequence: Number for every record (long: 8 bytes).
2. EntryDigest: A hash value used to map work (i.e. process record EntryDigest % AofReplaySubtaskCount == replayTaskId) to virtual replay tasks (byte: 1 byte)
3. For coordinated operations (e.g. Checkpoint, CustomTxn, Txn), we need to indicate which physical sublogs and which virtual replay tasks participate in the operation
   1. For physical sublog participation we simpley enqueue the corresponding marker to each physical sublog
   2. For virtual replay tasks, we have two options:
      1. Maintain a 256-bit map (i.e. 32 bytes) within the headers of the corresponding markers (i.e TxnStart, TxnCommit etc). For checkpointing all bits will be set since all tasks need to coordinate (i.e. wait for checkpoint to finish). This is probably the best option because it has a fixed overhead even when txn contains hundreds of keys.
      2. Maintain entry digest values for all keys participating in the coordinated operation (mainly for transactions, checkpoint does not need to track the keys).
   3. We also need a counter (virtualSublogAccessCount) to track total task participation across the physical sublogs to ensure all virtual replay task wait on barrier for all participants to complete the coordinated operation.

In essence, the system will operate as if it uses many virtual sublogs backed by a fixed number of physical sublogs.
The primary will write to a physical sublog by calculating the sublogIdx based on the command key specified at the operation.
The operation key will also be used to generate a single byte hash which will be used to determine how work is assinged to virtual replay tasks in the replica

Example:

Primary Run:
set foo 1
set wxz 2

hash1 = HashKey(foo)
hash2 = HashKey(wxz)

sublogIdx1 = hash1 % aofSublogCount
sublogIdx2 = hash2 % aofSublogCount

replayId1 = hash1 & 255
replayId2 = hash2 & 255

sublog[sublogIdx1].Enqueue(Header(replayIdx1),foo, 1)
sublog[sublogIdx2].Enqueue(Header(replayIdx2),wxz, 2)

Replica

if(!replayTaskInitialized){
  foreach(var taskId in range(replayTaskCount))
    ReplayTask(taskId, replayTaskCount)
}

void ReplayTask(int taskId, int replayTaskCount)
{
  while(true){
    var record = GetNext();
    var header = record.Header;
    if(header.replayIdx % replayTaskCount == taskId){
      ProcessRecord(record);
    }
  }
}


- Support virtual sublog replay for a single log
- Ensure that replay is possible with varying number of virtual sublogs (NOTE: physical sublogs should remain fixed)
- Every enqueue operation adds hash value (single byte) to use with assigning work to each virtual subtask
- Coordination for transactions adds a marker for every participating virtual sublog. That marker can be hashed to support varying number of virtual subtasks

NOTES:
- Coordinate replication offset update and maximum timestamp for single sublog

</ul></ul>

- [ ] Role command does not work as expected with SE Redis.
<ul><ul>
  Message: 
    Unexpected response to ROLE: Array: 3 items

  Stack Trace: 
    ClusterTestUtils.RoleCommand(IPEndPoint endPoint, ILogger logger) line 2623
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation) line 1554
    GenericAdapter`1.GetResult()
    AsyncToSyncAdapter.Await[TResult](Func`1 invoke)
    AsyncToSyncAdapter.Await(Func`1 invoke)
    TestMethodCommand.RunTestMethod(TestExecutionContext context)
    TestMethodCommand.Execute(TestExecutionContext context)
    <>c__DisplayClass1_0.<Execute>b__0()
    DelegatingTestCommand.RunTestMethodInThreadAbortSafeZone(TestExecutionContext context, Action action)
    1)    at Garnet.test.cluster.ClusterTestUtils.RoleCommand(IPEndPoint endPoint, ILogger logger) in C:\Dev\Github\vazois\test\Garnet.test.cluster\ClusterTestUtils.cs:line 2618
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation) line 1554
    AsyncMethodBuilderCore.Start[TStateMachine](TStateMachine& stateMachine)
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation)
</ul></ul>

- [ ] KEYS will return out of order result as observed by the PRIMARY vs the REPLICA since REPLICA replays SET in parallel.

- AofProcessor replay should use a separate basic context for replay?

- When running a custom transaction proc, we need to wait on a barrier for all replay tasks associated with that sublog.
  The dictionary holding the shared barriers is indexed on sessionID. Across sessions this should be fine. 
  However, for the same session it might create a problem because two separate transactions executing on the same session one after the other
  that operate on disjoint sublogs might get mixed because they will attempt to get indexed on the same sessionID.

- What if replica metadata (i.e checkpoint entry and aof address range) is not valid? Valid means that aofBeginAddress < checkpointCoveredAofAddress < aofEndAddress. This is probably a bug for main also.
- What happens when two write operations acquire the same timestamp?
<ul><ul>
Assume the following writes; a1,t1 and a2,t2
If the granularity of the clock is coarse (i.e. 10 sec) then it is possible for t1 == t2. In that case, we cannot differentiate on the ordering of a1 and a2 despite the fact that they might have happened one after the other.
In that situation, possible solutions include
1. Higher clock granularity/ unique sequence number generation. What is the scalability of this approach.
2. Include sessionId and dedicated counter per session to resolve ordering at the replica (i.e. consistent read) (ts, sid, ctr)
3. Use the concept of closing down timestamp. We only read at t-1 and read at t only when all sublogs have replayed t. Is that correct what are th caveats
</ul></ul>


<ul><ul>

### Prefix Consistent Recovery

In the single log case, issuing a Commit operation (with FastCommit enable), will result in persisting the log data into the disk up to that point
and including the Commit metadata.
Currently on recovery of a single log, Garnet will scan and recover its AOF log until the latest (flushed to disk) Commit record.
In the multi-log case, Commit operations happen in parallel for each individual sublog.
There is no guarantee that an issued commit operation will succeed for all logs.
In addition, even if the commit operation succeeds for all logs, we cannot replay all records blindly until the commit marker per sublog because that may result in a prefix inconsistent view of the database after recovery.

Consider the below sequence of operations as they occur in the single log case with commit markers added indicating a flush operation
On recovery, of that specific example, Garnet will replay all operations until c1 as they have been persisted on disk.
single log snapshot
<----------------------------->
[A, B, C, c1], D, E, F

When Garnet is configured with multi-log enabled, the corresponding operations may be recorded on any of the available sublogs
based on the hash function that is being used.
In the below example, the commit marker can be inserted in arbitrary position per log.
Following the legacy single log recover protocol, we would have recovered a snapshot of the database until the c1 marker for each individual sublog.
This will be an prefix inconsistent snapshot of the database, because operation E can be observed without D.

s1: [A, C, E, c1], F
s2: [B, c1], D

In order to avoid this scenario, we need to include a timestamp within the commit metadata to be able to resolve this scenario
by skipping on replay records with timestamps highers than the minimum timestamp established by the commit markers.

Including timestamps to the commit metadata is done through the CommitCookie.
The timestamp (Ts) needs to be calculated under epoch protection within each sublog to ensure that writers to log have had a chance to observe the Ts value, hence the associated flush operation would guarantee that all records generated with Ts' <= Ts would have been persisted into the disk.

There are two ways to issue commit operations on a TsavoriteLog instance.
1. Using an external API call using Commit/CommitAsync methods
2. Using an internal call to Commit by enabling the AutoCommit flag

In both cases, we need to ensure that Commit operations are issued in unison across all sublogs.
This is necessary because the recover process requires having observed a minimum timestamp across all sublogs to ensure prefix consistency.
More specifically on recovery, Garnet will acquire the latest commit metadata for every sublog and calculate the minimum timestamp across all.
Then it will scan through the log and replay only those records with timestamp equal or less to the recovered minimum commit timestamp
across all physical sublogs.
If a sublog misses a commit record, we cannot establish the upper bound until which all records have been persisted for that sublog.
There might be records created before the given timestamp that have not been flushed. Hence, not replaying them will recover the database into a prefix inconsistent state.

</ul></ul>

TODO:
- Do we have to change AofHeader version since we use padding bits?
- Do we need to wait for unsafe commit to complete when parallel replay within a sublog is enabled?
- Move enqueue operation within GarnetAppendOnlyFile if possible to simplify changes.
- Use SingleLog when 1 physical sublog and multiple replay tasks are used.
- Optimistically update timestamp at the end of a full page parallel replay.
- Revisit counting barrier.
- Refactor recover piece in AofProcessor to be separate from rest AofProcessor functionality.
- Use commit cookie to indicate a commit id and recover AOF until the point where all commit ids are consistent across all sublogs.