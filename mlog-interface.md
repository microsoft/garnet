This PR implements sharded-log support for Garnet replication.
This is a draft PR to keep track of the major TODOs still needed.

TODO:
- [X] Ensure that monotonicity of timestamps is fullfilled across failovers
<ul><ul>
What happens in the event the timestamp generated by a replica that became the new primary was behind the primary generated by the old primary?
</ul></ul>
- [X] Ensure replica replay task caches objects accessed during replay (i.e. tsavoriteLog instance)
<ul><ul>
A dedicated session is responsible for processing CLUSTER APPENDLOG commands at the replica.
For each session, we need to cache the log instance associated with the sublogIdx that is provided by the APPENDLOG command.
</ul></ul>

- [X] Build mechanism to coordinate dispose and reset of sublog replica replay task
<ul><ul>
Every sublog replay task will have to be able to signal and cancel other tasks from its group in the event of replication failure.
This requires a mechanism to inform other sublog replay tasks and ensure that they are shutdown or reset before making any new attempts to re-establish the replication stream 
</ul></ul>

- [X] Ensure that the timestampTracker that is used for the read protocol is reset appropriately when a node change its state.
<ul><ul>
This happens in the event a replica starts replicating a new primary
</ul></ul>

- [X] Ensure timestamp tracker recovers correctly alongside the sequence number generator
<ul><ul></ul></ul> 
- [X] Validate diskless replication tests and add new tests for replay coordination.
<ul><ul></ul></ul> 

- [X] Implement read protocol
<ul><ul>
The protocol needs to ensure prefix consistency for every session running on the replica.
This is achieved by waiting until the sequence number of the key being read is greater or equal to the maximum session sequence number.
However, this establishes only a lower bound for the actual sequence number. If an update occurs in between the moment the lower bound was established and before the actual read of the data, then the actual sequence number has been underestimated which violates the read protocol by advertising a less restrictive maximum sequence number for the session.
Hence, we update the maximum session sequence number after the actual read to ensure prefix consistency
This is enforced for the following operations

<ul><ul>
- [X] Basic context operations
</ul></ul> 
<ul><ul>
- [X] Scan operations
</ul></ul> 
<ul><ul>
- [X] Transaction operations
</ul></ul> 

The read protocol is implemented using the following

1. RespServerSession.ConsistentReadKeyPrepareCallback: Method to check key sequence number against the maximum session sequence number and wait for the former to catch up.
2. RespServerSession.ConsistentReadKeyPrepareCallback: Method to update maximum session sequence number with the key sequence number.
3. ConsistentReadContext: An extension to the BasicContext that overloads the Read methods, calling ConsistentReadKeyPrepareCallback before read and ConsistentReadKeyUpdateCallback after read
4. ConsistentReadDatabaseSession: A GarnetDatabaseSession instance initialized with the ConsistenReadContext instance

The main idea is to maintain two database sessions per RespServerSession instances. Primary nodes server read/writes on the default database session.
Replicas will use the ConsistentReadDatabaseSession to server consistent reads when sharded-log based AOF is enabled in cluster modes.
We require two distinct database sessions to allow any given node to switch back and forth between being a primary and a replica.
The primaries avoid evaluation of the consistent read protocol completely through this approach.

NOTES: 
1. The database session switch happens when the node transitions from primary to replica and vice versa.
This will happen at CLUSTER REPLICATE, REPLICAOF or CLUSTER FAILOVER

2. If the read goes pending the update happens after the complete pending completes.
This should guarantee prefix consistency even in the scatter-gather case.
This realies on the assumption that the read returns the unaltered disk value from the immutable region,
otherwise an RMW from the replay thread could interject an newer value which is not consistent according to the protocol.
Good thing is that this is what currently happens on the complete pending callback (Need to actually verify).

</ul></ul> 

- [X] Re-design tail witness to avoid using enqueue.
- [ ] Implement logical subtask replay (LSR)
<ul><ul>
Garnet replication can be configured to utilize multiple sublogs at the primary and many virtual replay tasks at the replica.
At minimum one can deploy Garnet with single log and single virtual replay task.
This case will be similar to what we have today and which does not need any additional information in the AofHeader.
For AofSublogCount > 1 and/or AofReplaySubtaskCount > 1, we need to make use AofExtendedHeader to store the following additional information

1. Timestamp/Sequence: Number for every record (long: 8 bytes).
2. EntryDigest: A hash value used to map work (i.e. process record EntryDigest % AofReplaySubtaskCount == replayTaskId) to virtual replay tasks (byte: 1 byte)
3. For coordinated operations (e.g. Checkpoint, CustomTxn, Txn), we need to indicate which physical sublogs and which virtual replay tasks participate in the operation
   1. For physical sublog participation we simpley enqueue the corresponding marker to each physical sublog
   2. For virtual replay tasks, we have two options:
      1. Maintain a 256-bit map (i.e. 32 bytes) within the headers of the corresponding markers (i.e TxnStart, TxnCommit etc). For checkpointing all bits will be set since all tasks need to coordinate (i.e. wait for checkpoint to finish). This is probably the best option because it has a fixed overhead even when txn contains hundreds of keys.
      2. Maintain entry digest values for all keys participating in the coordinated operation (mainly for transactions, checkpoint does not need to track the keys).
   3. We also need a counter (virtualSublogAccessCount) to track total task participation across the physical sublogs to ensure all virtual replay task wait on barrier for all participants to complete the coordinated operation.

In essence, the system will operate as if it uses many virtual sublogs backed by a fixed number of physical sublogs.
The primary will write to a physical sublog by calculating the sublogIdx based on the command key specified at the operation.
The operation key will also be used to generate a single byte hash which will be used to determine how work is assinged to virtual replay tasks in the replica

Example:

Primary Run:
set foo 1
set wxz 2

hash1 = HashKey(foo)
hash2 = HashKey(wxz)

sublogIdx1 = hash1 % aofSublogCount
sublogIdx2 = hash2 % aofSublogCount

replayId1 = hash1 & 255
replayId2 = hash2 & 255

sublog[sublogIdx1].Enqueue(Header(replayIdx1),foo, 1)
sublog[sublogIdx2].Enqueue(Header(replayIdx2),wxz, 2)

Replica

if(!replayTaskInitialized){
  foreach(var taskId in range(replayTaskCount))
    ReplayTask(taskId, replayTaskCount)
}

void ReplayTask(int taskId, int replayTaskCount)
{
  while(true){
    var record = GetNext();
    var header = record.Header;
    if(header.replayIdx % replayTaskCount == taskId){
      ProcessRecord(record);
    }
  }
}


- Support virtual sublog replay for a single log
- Ensure that replay is possible with varying number of virtual sublogs (NOTE: physical sublogs should remain fixed)
- Every enqueue operation adds hash value (single byte) to use with assigning work to each virtual subtask
- Coordination for transactions adds a marker for every participating virtual sublog. That marker can be hashed to support varying number of virtual subtasks
</ul></ul>

- [ ] Role command does not work as expected with SE Redis.
<ul><ul>
  Message: 
    Unexpected response to ROLE: Array: 3 items

  Stack Trace: 
    ClusterTestUtils.RoleCommand(IPEndPoint endPoint, ILogger logger) line 2623
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation) line 1554
    GenericAdapter`1.GetResult()
    AsyncToSyncAdapter.Await[TResult](Func`1 invoke)
    AsyncToSyncAdapter.Await(Func`1 invoke)
    TestMethodCommand.RunTestMethod(TestExecutionContext context)
    TestMethodCommand.Execute(TestExecutionContext context)
    <>c__DisplayClass1_0.<Execute>b__0()
    DelegatingTestCommand.RunTestMethodInThreadAbortSafeZone(TestExecutionContext context, Action action)
    1)    at Garnet.test.cluster.ClusterTestUtils.RoleCommand(IPEndPoint endPoint, ILogger logger) in C:\Dev\Github\vazois\test\Garnet.test.cluster\ClusterTestUtils.cs:line 2618
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation) line 1554
    AsyncMethodBuilderCore.Start[TStateMachine](TStateMachine& stateMachine)
    ClusterReplicationBaseTests.ReplicasRestartAsReplicasAsync(CancellationToken cancellation)
</ul></ul>

- [ ] KEYS will return out of order result as observed by the PRIMARY vs the REPLICA since REPLICA replays SET in parallel.
